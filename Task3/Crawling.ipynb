{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling data from internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Itunes clawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maksim/anaconda/envs/py3k/lib/python3.5/site-packages/sklearn/utils/fixes.py:64: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  if 'order' in inspect.getargspec(np.copy)[0]:\n"
     ]
    }
   ],
   "source": [
    "from lxml import html\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AppCrawler:\n",
    "    def __init__(self, starting_url, depth):\n",
    "        self.starting_url = starting_url\n",
    "        self.depth = depth\n",
    "        self.current_depth = 0\n",
    "        self.depth_links = []\n",
    "        self.apps = []\n",
    "        \n",
    "    def crawl(self):\n",
    "        app = self.get_app_from_link(self.starting_url)\n",
    "        self.apps.append(app)\n",
    "        self.depth_links.append(app.links)\n",
    "        while self.current_depth < self.depth:\n",
    "            current_links = []\n",
    "            for link in self.depth_links[self.current_depth]:\n",
    "                current_app = self.get_app_from_link(link)\n",
    "                current_links.extend(current_app.links)\n",
    "                self.apps.append(current_app)\n",
    "                time.sleep(5) #sleep for 5 secs\n",
    "            self.current_depth += 1\n",
    "            self.depth_links.append(current_links)\n",
    "    \n",
    "    def get_app_from_link(self, link):\n",
    "        start_page = requests.get(link)\n",
    "        #print(start_page.text)\n",
    "        tree = html.fromstring(start_page.text)\n",
    "        name = tree.xpath('//h1[@itemprop=\"name\"]/text()')[0]\n",
    "        developer = tree.xpath('//div[@class=\"left\"]/h2/text()')[0]\n",
    "        price = tree.xpath('//div[@itemprop=\"price\"]/text()')[0]\n",
    "        links = tree.xpath('//div[@class=\"center-stack\"]//*/a[@class=\"name\"]/@href')\n",
    "\n",
    "        app = App(name, developer, price, links)\n",
    "        return app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class App:\n",
    "    def __init__(self, name, developer, price, links):\n",
    "        self.name = name\n",
    "        self.developer = developer\n",
    "        self.price = price\n",
    "        self.links = links\n",
    "        \n",
    "    def print_app(self):\n",
    "        print(\"Name: \" + self.name + \n",
    "                \"\\r\\nDeveloper: \" + self.developer + \n",
    "                \"\\r\\nPrice: \" + self.price + \"\\r\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crawler = AppCrawler('https://itunes.apple.com/ru/app/candy-crush-saga/id553834731?l=en&mt=8', 1)\n",
    "crawler.crawl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Candy Crush Saga\r\n",
      "Developer: By King\r\n",
      "Price: Free\r\n",
      "\n",
      "Name: Juice Jam\r\n",
      "Developer: By SGN\r\n",
      "Price: Free\r\n",
      "\n",
      "Name: Bejeweled Blitz\r\n",
      "Developer: By PopCap\r\n",
      "Price: Free\r\n",
      "\n",
      "Name: Jewel Mania™\r\n",
      "Developer: By Storm8 Studios\r\n",
      "Price: Free\r\n",
      "\n",
      "Name: Diamond Dash\r\n",
      "Developer: By wooga\r\n",
      "Price: Free\r\n",
      "\n",
      "Name: Bubble Mania™ - Free Bubble Shooter\r\n",
      "Developer: By Storm8 Studios\r\n",
      "Price: Free\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for app in crawler.apps:\n",
    "    app.print_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class YandexNewsCrawler:\n",
    "    def __init__(self):\n",
    "        '''self.rubr_dict = {\"1politics\":  \"https://news.yandex.ru/politics.html?from=rubric\",\n",
    "                          \"1society\" :  \"https://news.yandex.ru/society.html?from=rubric\",\n",
    "                          \"1business\":  \"https://news.yandex.ru/business.html?from=rubric\",\n",
    "                          \"1world\":     \"https://news.yandex.ru/world.html?from=rubric\",\n",
    "                          \"sport\":     \"https://news.yandex.ru/sport.html?from=rubric\",\n",
    "                          \"incident\":  \"https://news.yandex.ru/incident.html?from=rubric\",\n",
    "                          \"1culture\":   \"https://news.yandex.ru/culture.html?from=rubric\",\n",
    "                          \"science\":   \"https://news.yandex.ru/science.html?from=rubric\",\n",
    "                          \"1computers\": \"https://news.yandex.ru/computers.html?from=rubric\",\n",
    "                          \"1auto\":      \"https://news.yandex.ru/auto.html?from=rubric\"}'''\n",
    "        \n",
    "        self.rubr_dict = {\n",
    "                          \"sport\":     \"https://news.yandex.ru/sport.html?from=rubric\",\n",
    "                          \"incident\":  \"https://news.yandex.ru/incident.html?from=rubric\",\n",
    "                          \"science\":   \"https://news.yandex.ru/science.html?from=rubric\"\n",
    "                         }\n",
    "        \n",
    "    def crawl(self):\n",
    "        for rubr_name, rubr_url in self.rubr_dict.items():\n",
    "            rubr = Rubric(rubr_name, rubr_url)\n",
    "            rubr.crawl()\n",
    "            print(\"Crawled \", rubr_name)\n",
    "            texts = np.array(rubr.texts)\n",
    "            np.save(\"./YandexNews10/\" + rubr_name, texts)\n",
    "            \n",
    "        \n",
    "class Rubric:\n",
    "    def __init__(self, rubr_name, rubr_url):\n",
    "        self.rubr_name = rubr_name\n",
    "        self.rubr_url = rubr_url\n",
    "        self.rubr_links = []\n",
    "        self.group_links = []\n",
    "        self.texts = []\n",
    "    \n",
    "    def crawl(self):\n",
    "        start_time = time.time()\n",
    "        self.rubr_links = self.parse_rubric_page(self.rubr_url)\n",
    "        for link in self.rubr_links:\n",
    "            article_links = self.parse_inside_chosen(link)\n",
    "            self.group_links.append(article_links)\n",
    "            for url in article_links:\n",
    "                text = self.parse_article(url)\n",
    "                self.texts.append(text)\n",
    "                time.sleep(5)\n",
    "        print(\"Crawling time: \", time.time() - start_time)\n",
    "\n",
    "        \n",
    "    def parse_rubric_page(self, link):\n",
    "        start_page = requests.get(link)\n",
    "        tree = html.fromstring(start_page.text)\n",
    "        links = tree.xpath('//div[@class=\"story__content\"]/h2/a/@href')\n",
    "        #descrs = tree.xpath('//div[@class=\"story__text\"]/text()')\n",
    "        return links\n",
    "    \n",
    "    def parse_inside_chosen(self, link):\n",
    "        base = \"https://news.yandex.ru\"\n",
    "        start_page = requests.get(base + link)\n",
    "        tree = html.fromstring(start_page.text)\n",
    "        times = tree.xpath('//div[@class=\"story__list\"]//*/div[@class=\"doc__time\"]/text()')\n",
    "        agencies = tree.xpath('//div[@class=\"story__list\"]//*/div[@class=\"doc__agency\"]/text()')\n",
    "        #story_list = tree.xpath('//div[@class=\"story__tabs-panes tabs-panes i-bem\"]//*/div[@class=\"story__list\"]')\n",
    "        article_links = tree.xpath('//div[@class=\"story__list\"]//*/div[@class=\"doc__head\"]/h2/a/@href')\n",
    "        #print(story_list)\n",
    "        return article_links[:1]\n",
    "    \n",
    "    def parse_article(self, link):\n",
    "        article_page = requests.get(link)\n",
    "        tree = html.fromstring(article_page.text)\n",
    "        p_tags = tree.xpath('//p/text()')\n",
    "        citates = tree.xpath('//blockquote/text()')\n",
    "        h1_texts = tree.xpath('//h1/text()')\n",
    "        h2_texts = tree.xpath('//h2/text()')\n",
    "        h3_texts = tree.xpath('//h3/text()')\n",
    "        article_text = p_tags + citates + h1_texts + h2_texts + h3_texts\n",
    "        return prepare_text(' '.join(article_text))\n",
    "\n",
    "    def prepare_text(self, text):\n",
    "        return stem_file(text)\n",
    "    \n",
    "    def stem_file(text):\n",
    "        text = re.sub(r\"(\\n)\", \" \", text.lower())\n",
    "        text = re.split(\"[^а-я]\", text)\n",
    "        morph = pymorphy2.MorphAnalyzer()\n",
    "        stemmed_text = []\n",
    "        for word in text:\n",
    "            if len(word) > 0:\n",
    "                stemmed_text.append(morph.parse(word)[0].normal_form)\n",
    "        stemmed_text = [word for word in stemmed_text if word not in stopwords.words(\"russian\")]\n",
    "        return stemmed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling time:  108.7225821018219\n",
      "Crawled  computers\n",
      "Crawling time:  101.08179903030396\n",
      "Crawled  society\n",
      "Crawling time:  114.44900393486023\n",
      "Crawled  culture\n",
      "Crawling time:  103.81476020812988\n",
      "Crawled  world\n",
      "Crawling time:  111.8690710067749\n",
      "Crawled  business\n",
      "Crawling time:  105.89873814582825\n",
      "Crawled  auto\n",
      "Crawling time:  105.34562516212463\n",
      "Crawled  politics\n",
      "Crawling time:  108.32536506652832\n",
      "Crawled  sport\n",
      "Crawling time:  108.7105770111084\n",
      "Crawled  science\n",
      "Crawling time:  102.34683895111084\n",
      "Crawled  incident\n"
     ]
    }
   ],
   "source": [
    "crawler = YandexNewsCrawler()\n",
    "crawler.crawl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
